{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# **EduSense: Intelligent Confusion Detection System**\n",
        "## YOLO + Vision Transformer + Kolmogorov-Arnold Networks\n",
        "\n",
        "**King Khalid University - College of Computer Science**  \n",
        "**Graduation Project 2025**\n",
        "\n",
        "---\n",
        "\n",
        "### Team Members:\n",
        "- Saeed Mohammed S Asiri (444810913)\n",
        "- Fahad Abdullah Ali AL-Qahtani (444802593)\n",
        "- Khalid Mushabbab Al-Dahwan (444803647)\n",
        "- Ahmad Turki Al Sultan (444803284)\n",
        "- Basil Hasan Al Muawwadh (442811409)\n",
        "\n",
        "**Supervisor:** Dr. Anand Deva Durai C\n",
        "\n",
        "---\n",
        "\n",
        "### Architecture Pipeline:\n",
        "```\n",
        "Video Frame ‚Üí YOLOv8 (Face Detection) ‚Üí ViT (Feature Extraction) ‚Üí \n",
        "Temporal Aggregation ‚Üí KAN (Confusion Classification) ‚Üí Confusion Curve\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section1"
      },
      "source": [
        "## üì¶ **1. Installation & Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q ultralytics transformers opencv-python scipy matplotlib seaborn tqdm\n",
        "!pip install -q torch torchvision --upgrade\n",
        "\n",
        "print(\"‚úÖ All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from scipy.interpolate import BSpline\n",
        "from scipy.signal import savgol_filter, find_peaks\n",
        "\n",
        "from transformers import ViTModel, ViTImageProcessor\n",
        "from ultralytics import YOLO\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import shutil\n",
        "\n",
        "path = kagglehub.dataset_download(\"olgaparfenova/daisee\")\n",
        "print(\"Downloaded to:\", path)\n",
        "\n",
        "src = path\n",
        "dst = \"/content/DAiSEE\"\n",
        "\n",
        "shutil.copytree(src, dst, dirs_exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(os.listdir(\"/content/DAiSEE\"))\n",
        "print(os.listdir(\"/content/DAiSEE/DAiSEE\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "DATA_ROOT = \"/content/DAiSEE/DAiSEE\"\n",
        "\n",
        "print(\"Root:\", os.listdir(DATA_ROOT))\n",
        "\n",
        "# DataSet folder\n",
        "dataset_path = os.path.join(DATA_ROOT, \"DataSet\")\n",
        "print(\"DataSet folder:\", os.listdir(dataset_path))\n",
        "\n",
        "# Count all subject folders inside DataSet\n",
        "subjects = os.listdir(dataset_path)\n",
        "print(\"Number of subject folders:\", len(subjects))\n",
        "\n",
        "# Example: count videos inside first subject\n",
        "first_subject = os.path.join(dataset_path, subjects[0])\n",
        "print(\"Example subject:\", subjects[0])\n",
        "print(\"Number of clips inside:\", len(os.listdir(first_subject)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_path = os.path.join(DATA_ROOT, \"DataSet\", \"Train\")\n",
        "\n",
        "print(\"Train subjects:\", len(os.listdir(train_path)))\n",
        "\n",
        "first_subject = os.listdir(train_path)[0]\n",
        "first_subject_path = os.path.join(train_path, first_subject)\n",
        "\n",
        "print(\"Example subject:\", first_subject)\n",
        "print(\"Clips inside:\", len(os.listdir(first_subject_path)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels_path = os.path.join(DATA_ROOT, \"Labels\", \"TrainLabels.csv\")\n",
        "labels = pd.read_csv(labels_path)\n",
        "\n",
        "print(labels.head())\n",
        "print(\"Total labels:\", len(labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section2"
      },
      "source": [
        "## üéØ **2. Component 1: YOLOv8 Face Detector**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yolo"
      },
      "outputs": [],
      "source": [
        "class YOLOFaceDetector:\n",
        "    \"\"\"\n",
        "    YOLOv8-based real-time face detection.\n",
        "    Detects faces, returns bounding boxes and crops.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_path='yolov8n.pt', conf_threshold=0.5, device='cuda'):\n",
        "        self.model = YOLO(model_path)\n",
        "        self.conf_threshold = conf_threshold\n",
        "        self.device = device\n",
        "        \n",
        "    def detect_faces(self, frame, return_crops=True, padding=0.2):\n",
        "        \"\"\"\n",
        "        Detect faces in frame.\n",
        "        \n",
        "        Args:\n",
        "            frame: Input frame (H, W, 3) BGR\n",
        "            return_crops: Return cropped faces\n",
        "            padding: Padding around bbox (0.2 = 20%)\n",
        "        \n",
        "        Returns:\n",
        "            List of detections with bboxes and crops\n",
        "        \"\"\"\n",
        "        results = self.model(frame, conf=self.conf_threshold, verbose=False)\n",
        "        \n",
        "        detections = []\n",
        "        \n",
        "        for result in results:\n",
        "            boxes = result.boxes\n",
        "            \n",
        "            for box in boxes:\n",
        "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "                conf = float(box.conf[0].cpu().numpy())\n",
        "                \n",
        "                detection = {\n",
        "                    'bbox': [int(x1), int(y1), int(x2), int(y2)],\n",
        "                    'confidence': conf\n",
        "                }\n",
        "                \n",
        "                # Add padding\n",
        "                if return_crops:\n",
        "                    bbox_padded = self._add_padding(frame, detection['bbox'], padding)\n",
        "                    detection['face_crop'] = self._crop_face(frame, bbox_padded)\n",
        "                    detection['bbox_padded'] = bbox_padded\n",
        "                \n",
        "                detections.append(detection)\n",
        "        \n",
        "        return detections\n",
        "    \n",
        "    def _add_padding(self, frame, bbox, padding):\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        h, w = frame.shape[:2]\n",
        "        \n",
        "        face_w = x2 - x1\n",
        "        face_h = y2 - y1\n",
        "        pad_w = int(face_w * padding)\n",
        "        pad_h = int(face_h * padding)\n",
        "        \n",
        "        x1_pad = max(0, x1 - pad_w)\n",
        "        y1_pad = max(0, y1 - pad_h)\n",
        "        x2_pad = min(w, x2 + pad_w)\n",
        "        y2_pad = min(h, y2 + pad_h)\n",
        "        \n",
        "        return [x1_pad, y1_pad, x2_pad, y2_pad]\n",
        "    \n",
        "    def _crop_face(self, frame, bbox):\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        return frame[y1:y2, x1:x2].copy()\n",
        "    \n",
        "    def get_largest_face(self, detections):\n",
        "        \"\"\"Return largest face (primary student)\"\"\"\n",
        "        if not detections:\n",
        "            return None\n",
        "        \n",
        "        areas = [(d['bbox'][2] - d['bbox'][0]) * (d['bbox'][3] - d['bbox'][1]) for d in detections]\n",
        "        return detections[np.argmax(areas)]\n",
        "\n",
        "\n",
        "# Test YOLO\n",
        "print(\"Initializing YOLOv8 Face Detector...\")\n",
        "yolo_detector = YOLOFaceDetector(device=device)\n",
        "print(\"‚úÖ YOLO ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section3"
      },
      "source": [
        "## üß† **3. Component 2: Vision Transformer Feature Extractor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vit"
      },
      "outputs": [],
      "source": [
        "class ViTFeatureExtractor:\n",
        "    \"\"\"\n",
        "    Vision Transformer for extracting facial features.\n",
        "    Uses pre-trained ViT from HuggingFace.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_name='google/vit-base-patch16-224-in21k', device='cuda'):\n",
        "        self.device = torch.device(device)\n",
        "        \n",
        "        print(f\"Loading Vision Transformer: {model_name}\")\n",
        "        self.processor = ViTImageProcessor.from_pretrained(model_name)\n",
        "        self.model = ViTModel.from_pretrained(model_name).to(self.device)\n",
        "        self.model.eval()\n",
        "        \n",
        "        self.embedding_dim = self.model.config.hidden_size\n",
        "        print(f\"‚úÖ ViT loaded | Embedding dim: {self.embedding_dim}\")\n",
        "    \n",
        "    def extract_features(self, face_image):\n",
        "        \"\"\"\n",
        "        Extract features from face crop.\n",
        "        \n",
        "        Args:\n",
        "            face_image: Face crop (H, W, 3) BGR or PIL Image\n",
        "        \n",
        "        Returns:\n",
        "            features: (embedding_dim,) numpy array\n",
        "        \"\"\"\n",
        "        # Convert BGR to RGB\n",
        "        if isinstance(face_image, np.ndarray):\n",
        "            face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\n",
        "            face_image = Image.fromarray(face_image)\n",
        "        \n",
        "        # Preprocess\n",
        "        inputs = self.processor(images=face_image, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "        \n",
        "        # Extract features\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            # Use [CLS] token\n",
        "            cls_token = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "        \n",
        "        return cls_token.squeeze()\n",
        "    \n",
        "    def extract_features_batch(self, face_images, batch_size=8):\n",
        "        \"\"\"\n",
        "        Batch feature extraction for efficiency.\n",
        "        \n",
        "        Args:\n",
        "            face_images: List of face crops\n",
        "            batch_size: Batch size\n",
        "        \n",
        "        Returns:\n",
        "            features: (num_faces, embedding_dim) array\n",
        "        \"\"\"\n",
        "        all_features = []\n",
        "        \n",
        "        for i in range(0, len(face_images), batch_size):\n",
        "            batch = face_images[i:i+batch_size]\n",
        "            \n",
        "            # Convert to PIL\n",
        "            batch_pil = []\n",
        "            for img in batch:\n",
        "                if isinstance(img, np.ndarray):\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                    img = Image.fromarray(img)\n",
        "                batch_pil.append(img)\n",
        "            \n",
        "            # Process batch\n",
        "            inputs = self.processor(images=batch_pil, return_tensors=\"pt\")\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                cls_tokens = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "            \n",
        "            all_features.append(cls_tokens)\n",
        "        \n",
        "        return np.vstack(all_features)\n",
        "\n",
        "\n",
        "# Initialize ViT\n",
        "vit_extractor = ViTFeatureExtractor(device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section4"
      },
      "source": [
        "## üî¨ **4. Component 3: Kolmogorov-Arnold Network (KAN)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kan_layer"
      },
      "outputs": [],
      "source": [
        "class KANLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Kolmogorov-Arnold Network Layer with learnable B-spline basis functions.\n",
        "    \n",
        "    Unlike traditional neural networks with fixed activations (ReLU, Sigmoid),\n",
        "    KAN learns the activation functions themselves as B-splines.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_features, out_features, num_basis=8, spline_order=3, grid_range=(-1, 1)):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.num_basis = num_basis\n",
        "        self.spline_order = spline_order\n",
        "        self.grid_range = grid_range\n",
        "        \n",
        "        # Learnable spline coefficients: (in_features, out_features, num_basis)\n",
        "        self.spline_coeffs = nn.Parameter(\n",
        "            torch.randn(in_features, out_features, num_basis) * 0.1\n",
        "        )\n",
        "        \n",
        "        # Create B-spline knot vector\n",
        "        num_knots = num_basis + spline_order + 1\n",
        "        internal_knots = num_basis - spline_order + 1\n",
        "        \n",
        "        knots = np.concatenate([\n",
        "            np.full(spline_order, grid_range[0]),\n",
        "            np.linspace(grid_range[0], grid_range[1], internal_knots),\n",
        "            np.full(spline_order, grid_range[1])\n",
        "        ])\n",
        "        \n",
        "        self.register_buffer('knots', torch.tensor(knots, dtype=torch.float32))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        \n",
        "        Args:\n",
        "            x: (batch_size, in_features)\n",
        "        \n",
        "        Returns:\n",
        "            (batch_size, out_features)\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # Normalize input to [-1, 1] using tanh\n",
        "        x_normalized = torch.tanh(x)\n",
        "        \n",
        "        # Evaluate B-spline basis\n",
        "        basis_values = self._evaluate_bspline_basis(x_normalized)\n",
        "        \n",
        "        # Apply coefficients: basis (b,i,k) √ó coeffs (i,o,k) ‚Üí output (b,o)\n",
        "        output = torch.einsum('bik,iok->bo', basis_values, self.spline_coeffs)\n",
        "        \n",
        "        return output\n",
        "    \n",
        "    def _evaluate_bspline_basis(self, x):\n",
        "        \"\"\"\n",
        "        Evaluate B-spline basis functions using Cox-de Boor recursion.\n",
        "        \n",
        "        Args:\n",
        "            x: (batch_size, in_features) values in [-1, 1]\n",
        "        \n",
        "        Returns:\n",
        "            (batch_size, in_features, num_basis)\n",
        "        \"\"\"\n",
        "        batch_size, in_features = x.shape\n",
        "        device = x.device\n",
        "        \n",
        "        # Initialize basis matrix\n",
        "        basis = torch.zeros(batch_size, in_features, self.num_basis, device=device)\n",
        "        \n",
        "        # Clamp to grid range\n",
        "        x_clamped = torch.clamp(x, self.grid_range[0], self.grid_range[1])\n",
        "        \n",
        "        # Simplified polynomial basis (for efficiency)\n",
        "        # In production, implement proper Cox-de Boor recursion\n",
        "        for k in range(self.num_basis):\n",
        "            # Polynomial powers: x^0, x^1, x^2, ...\n",
        "            basis[:, :, k] = x_clamped ** k\n",
        "        \n",
        "        # Normalize basis functions\n",
        "        basis = F.normalize(basis, p=2, dim=2)\n",
        "        \n",
        "        return basis\n",
        "\n",
        "\n",
        "# Test KAN Layer\n",
        "print(\"Testing KAN Layer...\")\n",
        "test_kan = KANLayer(in_features=10, out_features=5, num_basis=8)\n",
        "test_input = torch.randn(4, 10)\n",
        "test_output = test_kan(test_input)\n",
        "print(f\"Input shape: {test_input.shape} ‚Üí Output shape: {test_output.shape}\")\n",
        "print(\"‚úÖ KAN Layer working!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kan_model"
      },
      "outputs": [],
      "source": [
        "class ConfusionDetectorKAN(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Confusion Detection Model.\n",
        "    \n",
        "    Architecture:\n",
        "        ViT Features (768) ‚Üí Optional LSTM ‚Üí KAN Layers ‚Üí Confusion Score (0-1)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim=768, hidden_dims=[256, 128, 64], \n",
        "                 use_lstm=True, lstm_hidden=256, dropout=0.3, \n",
        "                 num_basis=8, spline_order=3):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.use_lstm = use_lstm\n",
        "        self.input_dim = input_dim\n",
        "        \n",
        "        # Optional LSTM for temporal modeling\n",
        "        if use_lstm:\n",
        "            self.lstm = nn.LSTM(\n",
        "                input_size=input_dim,\n",
        "                hidden_size=lstm_hidden,\n",
        "                num_layers=2,\n",
        "                batch_first=True,\n",
        "                bidirectional=True,\n",
        "                dropout=dropout\n",
        "            )\n",
        "            kan_input_dim = lstm_hidden * 2  # Bidirectional\n",
        "        else:\n",
        "            kan_input_dim = input_dim\n",
        "        \n",
        "        # KAN layers\n",
        "        self.kan_layers = nn.ModuleList()\n",
        "        \n",
        "        prev_dim = kan_input_dim\n",
        "        for hidden_dim in hidden_dims:\n",
        "            self.kan_layers.append(\n",
        "                KANLayer(prev_dim, hidden_dim, num_basis, spline_order)\n",
        "            )\n",
        "            prev_dim = hidden_dim\n",
        "        \n",
        "        # Final output layer\n",
        "        self.kan_output = KANLayer(prev_dim, 1, num_basis, spline_order)\n",
        "        \n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        # Output activation\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor\n",
        "               - If use_lstm=True: (batch, seq_len, 768)\n",
        "               - If use_lstm=False: (batch, 768)\n",
        "        \n",
        "        Returns:\n",
        "            confusion_score: (batch, 1) in [0, 1]\n",
        "        \"\"\"\n",
        "        # LSTM encoding\n",
        "        if self.use_lstm:\n",
        "            lstm_out, _ = self.lstm(x)  # (batch, seq_len, 512)\n",
        "            x = lstm_out[:, -1, :]  # Take last timestep (batch, 512)\n",
        "        \n",
        "        # KAN layers\n",
        "        for kan_layer in self.kan_layers:\n",
        "            x = kan_layer(x)\n",
        "            x = self.dropout(x)\n",
        "        \n",
        "        # Output\n",
        "        x = self.kan_output(x)\n",
        "        confusion_score = self.sigmoid(x)\n",
        "        \n",
        "        return confusion_score\n",
        "\n",
        "\n",
        "# Test full model\n",
        "print(\"\\nTesting Complete Confusion Detector...\")\n",
        "kan_model = ConfusionDetectorKAN(\n",
        "    input_dim=768,\n",
        "    hidden_dims=[256, 128, 64],\n",
        "    use_lstm=True\n",
        ").to(device)\n",
        "\n",
        "# Test with temporal sequence\n",
        "test_seq = torch.randn(4, 30, 768).to(device)  # (batch=4, seq_len=30, features=768)\n",
        "test_pred = kan_model(test_seq)\n",
        "print(f\"Input: {test_seq.shape} ‚Üí Output: {test_pred.shape}\")\n",
        "print(f\"Prediction range: [{test_pred.min():.3f}, {test_pred.max():.3f}]\")\n",
        "print(\"‚úÖ Full KAN model working!\")\n",
        "\n",
        "# Model summary\n",
        "total_params = sum(p.numel() for p in kan_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in kan_model.parameters() if p.requires_grad)\n",
        "print(f\"\\nModel Parameters:\")\n",
        "print(f\"  Total: {total_params:,}\")\n",
        "print(f\"  Trainable: {trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section5"
      },
      "source": [
        "## ‚è±Ô∏è **5. Component 4: Temporal Window Aggregator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "temporal"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "class TemporalWindowAggregator:\n",
        "    \"\"\"\n",
        "    Aggregate ViT features into temporal windows for KAN processing.\n",
        "    \n",
        "    Creates sliding windows of features (e.g., 3 seconds = 90 frames at 30fps)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, window_size=3.0, overlap=0.5, fps=30, feature_dim=768):\n",
        "        self.window_size = window_size\n",
        "        self.overlap = overlap\n",
        "        self.fps = fps\n",
        "        self.feature_dim = feature_dim\n",
        "        \n",
        "        # Window parameters\n",
        "        self.frames_per_window = int(window_size * fps)\n",
        "        self.stride = int(self.frames_per_window * (1 - overlap))\n",
        "        \n",
        "        # Buffer\n",
        "        self.feature_buffer = deque(maxlen=int(self.frames_per_window * 2))\n",
        "    \n",
        "    def add_frame(self, timestamp, features):\n",
        "        \"\"\"Add features for a single frame.\"\"\"\n",
        "        self.feature_buffer.append({\n",
        "            'timestamp': timestamp,\n",
        "            'features': features\n",
        "        })\n",
        "    \n",
        "    def get_windows(self):\n",
        "        \"\"\"\n",
        "        Extract all complete temporal windows.\n",
        "        \n",
        "        Returns:\n",
        "            List of window dicts with features and timestamps\n",
        "        \"\"\"\n",
        "        windows = []\n",
        "        buffer_list = list(self.feature_buffer)\n",
        "        \n",
        "        for i in range(0, len(buffer_list) - self.frames_per_window + 1, self.stride):\n",
        "            window_frames = buffer_list[i : i + self.frames_per_window]\n",
        "            \n",
        "            features_array = np.stack([f['features'] for f in window_frames])\n",
        "            \n",
        "            windows.append({\n",
        "                'start_time': window_frames[0]['timestamp'],\n",
        "                'end_time': window_frames[-1]['timestamp'],\n",
        "                'mid_time': (window_frames[0]['timestamp'] + window_frames[-1]['timestamp']) // 2,\n",
        "                'features': features_array,  # (num_frames, 768)\n",
        "                'num_frames': len(window_frames)\n",
        "            })\n",
        "        \n",
        "        return windows\n",
        "\n",
        "\n",
        "# Test aggregator\n",
        "print(\"Testing Temporal Aggregator...\")\n",
        "aggregator = TemporalWindowAggregator(window_size=3.0, overlap=0.5, fps=30)\n",
        "\n",
        "# Simulate adding frames\n",
        "for i in range(100):\n",
        "    timestamp = i * 33  # ~30 fps (33ms per frame)\n",
        "    features = np.random.randn(768)\n",
        "    aggregator.add_frame(timestamp, features)\n",
        "\n",
        "windows = aggregator.get_windows()\n",
        "print(f\"Generated {len(windows)} windows\")\n",
        "print(f\"First window: {windows[0]['start_time']}ms - {windows[0]['end_time']}ms\")\n",
        "print(f\"Window features shape: {windows[0]['features'].shape}\")\n",
        "print(\"‚úÖ Temporal aggregator working!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section6"
      },
      "source": [
        "## üîó **6. Complete End-to-End Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pipeline"
      },
      "outputs": [],
      "source": [
        "class ConfusionDetectionPipeline:\n",
        "    \"\"\"\n",
        "    Complete end-to-end confusion detection pipeline.\n",
        "    \n",
        "    Usage:\n",
        "        pipeline = ConfusionDetectionPipeline()\n",
        "        for frame, timestamp in video:\n",
        "            confusion_score = pipeline.process_frame(frame, timestamp)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, yolo_detector, vit_extractor, kan_model, \n",
        "                 window_size=3.0, fps=30, device='cuda'):\n",
        "        self.yolo = yolo_detector\n",
        "        self.vit = vit_extractor\n",
        "        self.kan = kan_model\n",
        "        self.device = device\n",
        "        \n",
        "        # Temporal aggregator\n",
        "        self.aggregator = TemporalWindowAggregator(\n",
        "            window_size=window_size,\n",
        "            fps=fps,\n",
        "            feature_dim=vit_extractor.embedding_dim\n",
        "        )\n",
        "        \n",
        "        # Results storage\n",
        "        self.confusion_scores = []\n",
        "        \n",
        "        # Set KAN to eval mode\n",
        "        self.kan.eval()\n",
        "    \n",
        "    def process_frame(self, frame, timestamp):\n",
        "        \"\"\"\n",
        "        Process a single frame.\n",
        "        \n",
        "        Args:\n",
        "            frame: Video frame (H, W, 3) BGR\n",
        "            timestamp: Timestamp in milliseconds\n",
        "        \n",
        "        Returns:\n",
        "            confusion_score: 0-1 value or None if no face\n",
        "        \"\"\"\n",
        "        # Step 1: Detect face\n",
        "        detections = self.yolo.detect_faces(frame, return_crops=True)\n",
        "        \n",
        "        if not detections:\n",
        "            return None\n",
        "        \n",
        "        # Use largest face\n",
        "        face_data = self.yolo.get_largest_face(detections)\n",
        "        face_crop = face_data['face_crop']\n",
        "        \n",
        "        # Step 2: Extract ViT features\n",
        "        features = self.vit.extract_features(face_crop)\n",
        "        \n",
        "        # Step 3: Add to temporal buffer\n",
        "        self.aggregator.add_frame(timestamp, features)\n",
        "        \n",
        "        # Step 4: Get windows and predict\n",
        "        windows = self.aggregator.get_windows()\n",
        "        \n",
        "        confusion_score = None\n",
        "        \n",
        "        if windows:\n",
        "            # Get latest window\n",
        "            latest_window = windows[-1]\n",
        "            \n",
        "            # Convert to tensor\n",
        "            window_features = torch.tensor(\n",
        "                latest_window['features'], \n",
        "                dtype=torch.float32\n",
        "            ).unsqueeze(0).to(self.device)  # (1, seq_len, 768)\n",
        "            \n",
        "            # KAN inference\n",
        "            with torch.no_grad():\n",
        "                confusion_score = self.kan(window_features).item()\n",
        "            \n",
        "            # Store\n",
        "            self.confusion_scores.append({\n",
        "                'timestamp': latest_window['mid_time'],\n",
        "                'score': confusion_score,\n",
        "                'window_start': latest_window['start_time'],\n",
        "                'window_end': latest_window['end_time']\n",
        "            })\n",
        "        \n",
        "        return confusion_score\n",
        "    \n",
        "    def get_confusion_curve(self):\n",
        "        \"\"\"Get complete confusion curve (timestamps, scores).\"\"\"\n",
        "        if not self.confusion_scores:\n",
        "            return [], []\n",
        "        \n",
        "        timestamps = [cs['timestamp'] for cs in self.confusion_scores]\n",
        "        scores = [cs['score'] for cs in self.confusion_scores]\n",
        "        \n",
        "        return timestamps, scores\n",
        "    \n",
        "    def detect_confusion_peaks(self, prominence=0.2, distance=10):\n",
        "        \"\"\"\n",
        "        Detect peaks in confusion curve.\n",
        "        \n",
        "        Args:\n",
        "            prominence: Minimum prominence of peaks\n",
        "            distance: Minimum distance between peaks (in data points)\n",
        "        \n",
        "        Returns:\n",
        "            List of peak dicts with timestamps and scores\n",
        "        \"\"\"\n",
        "        timestamps, scores = self.get_confusion_curve()\n",
        "        \n",
        "        if len(scores) < 3:\n",
        "            return []\n",
        "        \n",
        "        # Find peaks\n",
        "        peaks, properties = find_peaks(\n",
        "            scores, \n",
        "            prominence=prominence, \n",
        "            distance=distance\n",
        "        )\n",
        "        \n",
        "        peak_events = []\n",
        "        for peak_idx in peaks:\n",
        "            peak_events.append({\n",
        "                'timestamp': timestamps[peak_idx],\n",
        "                'score': scores[peak_idx],\n",
        "                'index': peak_idx\n",
        "            })\n",
        "        \n",
        "        return peak_events\n",
        "\n",
        "\n",
        "print(\"‚úÖ Complete pipeline ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section7"
      },
      "source": [
        "## üìä **7. Dataset Preparation & Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset"
      },
      "outputs": [],
      "source": [
        "class VideoConfusionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for training confusion detection.\n",
        "    Extracts temporal sequences from videos.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, video_paths, labels, yolo_detector, vit_extractor, \n",
        "                 max_frames=30, frame_skip=5):\n",
        "        self.video_paths = video_paths\n",
        "        self.labels = labels\n",
        "        self.yolo = yolo_detector\n",
        "        self.vit = vit_extractor\n",
        "        self.max_frames = max_frames\n",
        "        self.frame_skip = frame_skip\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.video_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        # Extract sequence\n",
        "        sequence = self._extract_sequence(video_path)\n",
        "        \n",
        "        return torch.tensor(sequence).float(), torch.tensor([label]).float()\n",
        "    \n",
        "    def _extract_sequence(self, video_path):\n",
        "        \"\"\"Extract ViT features from video.\"\"\"\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        embeddings = []\n",
        "        frame_idx = 0\n",
        "        \n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            \n",
        "            if frame_idx % self.frame_skip == 0:\n",
        "                # Detect face\n",
        "                detections = self.yolo.detect_faces(frame, return_crops=True)\n",
        "                \n",
        "                if detections:\n",
        "                    face = self.yolo.get_largest_face(detections)['face_crop']\n",
        "                    features = self.vit.extract_features(face)\n",
        "                    embeddings.append(features)\n",
        "            \n",
        "            frame_idx += 1\n",
        "            \n",
        "            if len(embeddings) >= self.max_frames:\n",
        "                break\n",
        "        \n",
        "        cap.release()\n",
        "        \n",
        "        # Handle empty or short sequences\n",
        "        if len(embeddings) == 0:\n",
        "            embeddings = [np.zeros(self.vit.embedding_dim)]\n",
        "        \n",
        "        # Pad to max_frames\n",
        "        sequence = np.array(embeddings)\n",
        "        if len(sequence) < self.max_frames:\n",
        "            pad_size = self.max_frames - len(sequence)\n",
        "            pad = np.zeros((pad_size, self.vit.embedding_dim))\n",
        "            sequence = np.vstack([sequence, pad])\n",
        "        \n",
        "        return sequence\n",
        "\n",
        "\n",
        "print(\"‚úÖ Dataset class ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training"
      },
      "outputs": [],
      "source": [
        "def train_confusion_detector(model, train_loader, val_loader, \n",
        "                             num_epochs=10, learning_rate=0.001, device='cuda'):\n",
        "    \"\"\"\n",
        "    Train confusion detection model.\n",
        "    \n",
        "    Args:\n",
        "        model: ConfusionDetectorKAN\n",
        "        train_loader: Training DataLoader\n",
        "        val_loader: Validation DataLoader\n",
        "        num_epochs: Number of training epochs\n",
        "        learning_rate: Learning rate\n",
        "        device: Device to train on\n",
        "    \n",
        "    Returns:\n",
        "        Trained model, training history\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # Optimizer and loss\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.BCELoss()\n",
        "    \n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'val_accuracy': []\n",
        "    }\n",
        "    \n",
        "    best_val_loss = float('inf')\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        \n",
        "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
        "        for sequences, labels in train_bar:\n",
        "            sequences = sequences.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            predictions = model(sequences)\n",
        "            loss = criterion(predictions, labels)\n",
        "            \n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            train_bar.set_postfix({'loss': loss.item()})\n",
        "        \n",
        "        train_loss /= len(train_loader)\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
        "            for sequences, labels in val_bar:\n",
        "                sequences = sequences.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                predictions = model(sequences)\n",
        "                loss = criterion(predictions, labels)\n",
        "                \n",
        "                val_loss += loss.item()\n",
        "                \n",
        "                # Accuracy (threshold at 0.5)\n",
        "                pred_labels = (predictions > 0.5).float()\n",
        "                correct += (pred_labels == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "        \n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy = correct / total\n",
        "        \n",
        "        # Update history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_accuracy'].append(val_accuracy)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
        "        print(f\"  Val Accuracy: {val_accuracy:.4f}\")\n",
        "        \n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_confusion_detector.pth')\n",
        "            print(\"  ‚úÖ Best model saved!\")\n",
        "        \n",
        "        print()\n",
        "    \n",
        "    return model, history\n",
        "\n",
        "\n",
        "print(\"‚úÖ Training function ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section8"
      },
      "source": [
        "## üé¨ **8. Demo: Process Video & Generate Confusion Curve**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "demo"
      },
      "outputs": [],
      "source": [
        "def process_video_demo(video_path, pipeline, output_path='confusion_curve.png'):\n",
        "    \"\"\"\n",
        "    Process a video and generate confusion curve.\n",
        "    \n",
        "    Args:\n",
        "        video_path: Path to video file\n",
        "        pipeline: ConfusionDetectionPipeline instance\n",
        "        output_path: Path to save confusion curve plot\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    \n",
        "    print(f\"Processing video: {video_path}\")\n",
        "    print(f\"FPS: {fps}, Total frames: {total_frames}\")\n",
        "    \n",
        "    frame_idx = 0\n",
        "    \n",
        "    with tqdm(total=total_frames, desc=\"Processing frames\") as pbar:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            \n",
        "            timestamp = int((frame_idx / fps) * 1000)  # milliseconds\n",
        "            \n",
        "            # Process frame\n",
        "            confusion_score = pipeline.process_frame(frame, timestamp)\n",
        "            \n",
        "            frame_idx += 1\n",
        "            pbar.update(1)\n",
        "            \n",
        "            if confusion_score is not None:\n",
        "                pbar.set_postfix({'confusion': f\"{confusion_score:.2f}\"})\n",
        "    \n",
        "    cap.release()\n",
        "    \n",
        "    # Get confusion curve\n",
        "    timestamps, scores = pipeline.get_confusion_curve()\n",
        "    \n",
        "    if not timestamps:\n",
        "        print(\"No confusion data generated (no faces detected?)\")\n",
        "        return\n",
        "    \n",
        "    # Detect peaks\n",
        "    peaks = pipeline.detect_confusion_peaks(prominence=0.2, distance=10)\n",
        "    \n",
        "    # Plot confusion curve\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    \n",
        "    # Convert timestamps to seconds\n",
        "    time_sec = [t/1000 for t in timestamps]\n",
        "    \n",
        "    # Plot curve\n",
        "    plt.plot(time_sec, scores, 'b-', linewidth=2, label='Confusion Score')\n",
        "    \n",
        "    # Mark peaks\n",
        "    if peaks:\n",
        "        peak_times = [p['timestamp']/1000 for p in peaks]\n",
        "        peak_scores = [p['score'] for p in peaks]\n",
        "        plt.scatter(peak_times, peak_scores, color='red', s=100, \n",
        "                   marker='o', zorder=5, label='Confusion Peaks')\n",
        "    \n",
        "    # Threshold line\n",
        "    plt.axhline(y=0.7, color='r', linestyle='--', alpha=0.5, label='High Confusion Threshold')\n",
        "    \n",
        "    plt.xlabel('Time (seconds)', fontsize=12)\n",
        "    plt.ylabel('Confusion Score', fontsize=12)\n",
        "    plt.title('EduSense Confusion Detection Curve', fontsize=14, fontweight='bold')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.ylim([0, 1])\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"\\n‚úÖ Confusion curve saved to: {output_path}\")\n",
        "    \n",
        "    # Statistics\n",
        "    print(f\"\\nConfusion Statistics:\")\n",
        "    print(f\"  Total windows: {len(scores)}\")\n",
        "    print(f\"  Average confusion: {np.mean(scores):.3f}\")\n",
        "    print(f\"  Max confusion: {np.max(scores):.3f} at {time_sec[np.argmax(scores)]:.1f}s\")\n",
        "    print(f\"  Detected peaks: {len(peaks)}\")\n",
        "    \n",
        "    if peaks:\n",
        "        print(f\"\\n  Peak timestamps (seconds):\")\n",
        "        for i, peak in enumerate(peaks[:5], 1):\n",
        "            print(f\"    {i}. {peak['timestamp']/1000:.1f}s (score: {peak['score']:.3f})\")\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    return timestamps, scores, peaks\n",
        "\n",
        "\n",
        "print(\"‚úÖ Demo function ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section9"
      },
      "source": [
        "## üöÄ **9. Initialize Complete Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_pipeline"
      },
      "outputs": [],
      "source": [
        "# Initialize all components\n",
        "print(\"Initializing EduSense Complete Pipeline...\\n\")\n",
        "\n",
        "# Initialize pipeline\n",
        "edusense_pipeline = ConfusionDetectionPipeline(\n",
        "    yolo_detector=yolo_detector,\n",
        "    vit_extractor=vit_extractor,\n",
        "    kan_model=kan_model,\n",
        "    window_size=3.0,\n",
        "    fps=30,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ EduSense Pipeline Ready!\")\n",
        "print(\"\\nComponents:\")\n",
        "print(\"  ‚úì YOLOv8 Face Detector\")\n",
        "print(\"  ‚úì Vision Transformer (768-dim embeddings)\")\n",
        "print(\"  ‚úì KAN Confusion Classifier\")\n",
        "print(\"  ‚úì Temporal Window Aggregator (3-second windows)\")\n",
        "print(\"\\nPipeline: Frame ‚Üí YOLO ‚Üí ViT ‚Üí Windows ‚Üí KAN ‚Üí Confusion Score\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section10"
      },
      "source": [
        "## üìπ **10. Test on Sample Video**\n",
        "\n",
        "Upload a video or use a dataset video to test the complete pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test"
      },
      "outputs": [],
      "source": [
        "# Example: Process a video\n",
        "# Replace 'path/to/video.mp4' with your actual video path\n",
        "\n",
        "# If you have a video file:\n",
        "# video_path = '/content/sample_lecture.mp4'\n",
        "# results = process_video_demo(video_path, edusense_pipeline)\n",
        "\n",
        "# For now, print instructions\n",
        "print(\"To test the pipeline:\")\n",
        "print(\"1. Upload a video file to Colab\")\n",
        "print(\"2. Set video_path = '/content/your_video.mp4'\")\n",
        "print(\"3. Run: process_video_demo(video_path, edusense_pipeline)\")\n",
        "print(\"\\nThe system will:\")\n",
        "print(\"  - Detect faces in each frame (YOLO)\")\n",
        "print(\"  - Extract features (ViT)\")\n",
        "print(\"  - Aggregate into 3-second windows\")\n",
        "print(\"  - Predict confusion scores (KAN)\")\n",
        "print(\"  - Generate confusion curve with peaks\")\n",
        "print(\"  - Save visualization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section11"
      },
      "source": [
        "## üíæ **11. Save & Export Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save"
      },
      "outputs": [],
      "source": [
        "# Save trained KAN model\n",
        "def save_edusense_model(kan_model, save_path='edusense_kan_model.pth'):\n",
        "    torch.save({\n",
        "        'model_state_dict': kan_model.state_dict(),\n",
        "        'model_config': {\n",
        "            'input_dim': kan_model.input_dim,\n",
        "            'use_lstm': kan_model.use_lstm,\n",
        "        }\n",
        "    }, save_path)\n",
        "    print(f\"‚úÖ Model saved to: {save_path}\")\n",
        "\n",
        "# Load model\n",
        "def load_edusense_model(load_path='edusense_kan_model.pth', device='cuda'):\n",
        "    checkpoint = torch.load(load_path, map_location=device)\n",
        "    \n",
        "    model = ConfusionDetectorKAN(\n",
        "        input_dim=checkpoint['model_config']['input_dim'],\n",
        "        use_lstm=checkpoint['model_config']['use_lstm']\n",
        "    ).to(device)\n",
        "    \n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "    \n",
        "    print(f\"‚úÖ Model loaded from: {load_path}\")\n",
        "    return model\n",
        "\n",
        "# Save current model\n",
        "save_edusense_model(kan_model, 'edusense_kan_initial.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section12"
      },
      "source": [
        "## üìä **12. Visualizations & Interpretability**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viz"
      },
      "outputs": [],
      "source": [
        "def visualize_kan_splines(kan_model, layer_idx=0, num_plots=4):\n",
        "    \"\"\"\n",
        "    Visualize learned spline functions in KAN layer.\n",
        "    This shows interpretability of the model.\n",
        "    \"\"\"\n",
        "    kan_layer = kan_model.kan_layers[layer_idx]\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    x_vals = np.linspace(-1, 1, 100)\n",
        "    \n",
        "    for plot_idx in range(num_plots):\n",
        "        ax = axes[plot_idx]\n",
        "        \n",
        "        # Select random input-output pair\n",
        "        in_idx = plot_idx % kan_layer.in_features\n",
        "        out_idx = plot_idx % kan_layer.out_features\n",
        "        \n",
        "        # Get spline coefficients\n",
        "        coeffs = kan_layer.spline_coeffs[in_idx, out_idx, :].detach().cpu().numpy()\n",
        "        \n",
        "        # Evaluate spline\n",
        "        y_vals = []\n",
        "        for x in x_vals:\n",
        "            # Simplified polynomial evaluation\n",
        "            y = sum(coeffs[k] * (x ** k) for k in range(len(coeffs)))\n",
        "            y_vals.append(y)\n",
        "        \n",
        "        ax.plot(x_vals, y_vals, 'b-', linewidth=2)\n",
        "        ax.set_title(f'Spline Function: Input {in_idx} ‚Üí Output {out_idx}', fontsize=10)\n",
        "        ax.set_xlabel('Input Value')\n",
        "        ax.set_ylabel('Activation')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.suptitle(f'KAN Layer {layer_idx} - Learned Spline Activations', \n",
        "                 fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('kan_splines_visualization.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"‚úÖ KAN splines visualization saved!\")\n",
        "\n",
        "# Visualize\n",
        "visualize_kan_splines(kan_model, layer_idx=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## üéì **Summary**\n",
        "\n",
        "### **What We Built:**\n",
        "\n",
        "1. **YOLOv8 Face Detector** - Real-time face detection (30+ fps)\n",
        "2. **Vision Transformer** - Extract 768-dim semantic features from faces\n",
        "3. **Kolmogorov-Arnold Network** - Interpretable confusion classification\n",
        "4. **Temporal Window Aggregator** - Create 3-second sliding windows\n",
        "5. **Complete Pipeline** - End-to-end confusion detection system\n",
        "\n",
        "### **Architecture:**\n",
        "```\n",
        "Video Frame (1280x720)\n",
        "        ‚Üì\n",
        "    [YOLOv8]  ‚Üê Face Detection\n",
        "        ‚Üì\n",
        "  Face Crop (224x224)\n",
        "        ‚Üì\n",
        "    [ViT]  ‚Üê Feature Extraction (768-dim)\n",
        "        ‚Üì\n",
        "  Temporal Windows (90 frames √ó 768 features)\n",
        "        ‚Üì\n",
        "    [LSTM + KAN]  ‚Üê Confusion Classification\n",
        "        ‚Üì\n",
        "  Confusion Score (0-1)\n",
        "```\n",
        "\n",
        "### **Key Features:**\n",
        "- ‚úÖ Real-time processing capable\n",
        "- ‚úÖ Interpretable predictions (KAN splines)\n",
        "- ‚úÖ Temporal modeling (LSTM + sliding windows)\n",
        "- ‚úÖ Peak detection for confusion hotspots\n",
        "- ‚úÖ Visualization & analysis tools\n",
        "\n",
        "### **Next Steps:**\n",
        "1. Train on labeled confusion dataset (DAiSEE or custom)\n",
        "2. Optimize hyperparameters\n",
        "3. Deploy in Individual Study Mode\n",
        "4. Integrate with RAG pipeline for adaptive remediation\n",
        "5. Build instructor dashboard for Class Analytics Mode\n",
        "\n",
        "---\n",
        "\n",
        "**King Khalid University - College of Computer Science**  \n",
        "**EduSense Graduation Project 2025**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
