{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# **EduSense: Production-Ready 2-Stage Architecture**\n",
        "\n",
        "**King Khalid University - Graduation Project 2025**\n",
        "\n",
        "---\n",
        "\n",
        "## Architecture Overview\n",
        "\n",
        "### **Stage A: Offline Feature Extraction** (Run Once)\n",
        "```\n",
        "Video â†’ YOLO (face detection) â†’ ViT (frozen features) â†’ Save .npy embeddings\n",
        "```\n",
        "\n",
        "### **Stage B: Training & Inference**\n",
        "```\n",
        "Load .npy â†’ LSTM (temporal modeling) â†’ KAN (classification) â†’ Confusion Score\n",
        "```\n",
        "\n",
        "**Key Improvements:**\n",
        "- âœ… Feature extraction decoupled from training\n",
        "- âœ… ViT frozen (no backprop through transformer)\n",
        "- âœ… Fast training (no video I/O)\n",
        "- âœ… Easy deployment (embeddings can be pre-computed)\n",
        "- âœ… Production-ready modular code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ‘¶**startign phase 0**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'kagglehub'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkagglehub\u001b[39;00m\n\u001b[32m      2\u001b[39m path = kagglehub.dataset_download(\u001b[33m\"\u001b[39m\u001b[33molgaparfenova/daisee\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'kagglehub'"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"olgaparfenova/daisee\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Downloaded to:\", path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "src = path\n",
        "dst = \"/content/DAiSEE\"\n",
        "\n",
        "shutil.copytree(src, dst, dirs_exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(os.listdir(\"/content/DAiSEE\"))\n",
        "print(os.listdir(\"/content/DAiSEE/DAiSEE\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(os.listdir(\"/content/DAiSEE\"))\n",
        "print(os.listdir(\"/content/DAiSEE/DAiSEE\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "train_path = os.path.join(DATA_ROOT, \"DataSet\", \"Train\")\n",
        "\n",
        "print(\"Train subjects:\", len(os.listdir(train_path)))\n",
        "\n",
        "first_subject = os.listdir(train_path)[0]\n",
        "first_subject_path = os.path.join(train_path, first_subject)\n",
        "\n",
        "print(\"Example subject:\", first_subject)\n",
        "print(\"Clips inside:\", len(os.listdir(first_subject_path)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DAiSEEDataset(Dataset):\n",
        "    def __init__(self, root, split=\"Train\", max_frames=30, frame_skip=5):\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.max_frames = max_frames\n",
        "        self.frame_skip = frame_skip\n",
        "        \n",
        "        self.video_root = os.path.join(root, \"DataSet\", split)\n",
        "        self.labels = pd.read_csv(os.path.join(root, \"Labels\", f\"{split}Labels.csv\"))\n",
        "        \n",
        "        self.samples = []\n",
        "        \n",
        "        for _, row in self.labels.iterrows():\n",
        "            clip_id = row[\"ClipID\"]\n",
        "            label = row[\"Engagement\"]\n",
        "            \n",
        "            subject_id = clip_id[:6]\n",
        "            clip_folder = clip_id.replace(\".avi\", \"\")\n",
        "            \n",
        "            video_path = os.path.join(\n",
        "                self.video_root,\n",
        "                subject_id,\n",
        "                clip_folder,\n",
        "                clip_id\n",
        "            )\n",
        "            \n",
        "            if os.path.exists(video_path):\n",
        "                self.samples.append((video_path, label))\n",
        "        \n",
        "        print(f\"{split} samples loaded:\", len(self.samples))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path, label = self.samples[idx]\n",
        "        \n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "        frame_idx = 0\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if frame_idx % self.frame_skip == 0:\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frame = cv2.resize(frame, (224, 224))\n",
        "                frame = torch.tensor(frame).permute(2,0,1).float() / 255.0\n",
        "                frames.append(frame)\n",
        "\n",
        "            frame_idx += 1\n",
        "\n",
        "            if len(frames) >= self.max_frames:\n",
        "                break\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        if len(frames) == 0:\n",
        "            frames = [torch.zeros(3, 224, 224)]\n",
        "\n",
        "        # Padding\n",
        "        if len(frames) < self.max_frames:\n",
        "            pad = [torch.zeros(3,224,224)] * (self.max_frames - len(frames))\n",
        "            frames.extend(pad)\n",
        "\n",
        "        sequence = torch.stack(frames)  # (T, 3, 224, 224)\n",
        "\n",
        "        return sequence, torch.tensor([label/3.0]).float()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = DAiSEEDataset(DATA_ROOT, split=\"Train\")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,   # small first\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"Dataset ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section1"
      },
      "source": [
        "## ðŸ“¦ **1. Installation & Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q ultralytics transformers opencv-python scipy matplotlib seaborn tqdm\n",
        "!pip install -q torch torchvision --upgrade\n",
        "\n",
        "print(\"âœ… All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section2"
      },
      "source": [
        "## ðŸ“š **2. Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import json\n",
        "from scipy.interpolate import BSpline\n",
        "\n",
        "from transformers import ViTModel, ViTImageProcessor\n",
        "from ultralytics import YOLO\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section3"
      },
      "source": [
        "## ðŸŽ¯ **3. YOLO Face Detector** (Kept from original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yolo"
      },
      "outputs": [],
      "source": [
        "class YOLOFaceDetector:\n",
        "    \"\"\"\n",
        "    YOLOv8-based real-time face detection.\n",
        "    Used ONLY in Stage A (offline extraction).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path='yolov8n.pt', conf_threshold=0.5, device='cuda'):\n",
        "        self.model = YOLO(model_path)\n",
        "        self.conf_threshold = conf_threshold\n",
        "        self.device = device\n",
        "\n",
        "    def detect_faces(self, frame, return_crops=True, padding=0.2):\n",
        "        \"\"\"\n",
        "        Detect faces in frame.\n",
        "\n",
        "        Args:\n",
        "            frame: Input frame (H, W, 3) BGR\n",
        "            return_crops: Return cropped faces\n",
        "            padding: Padding around bbox (0.2 = 20%)\n",
        "\n",
        "        Returns:\n",
        "            List of detections with bboxes and crops\n",
        "        \"\"\"\n",
        "        results = self.model(frame, conf=self.conf_threshold, verbose=False)\n",
        "\n",
        "        detections = []\n",
        "\n",
        "        for result in results:\n",
        "            boxes = result.boxes\n",
        "\n",
        "            for box in boxes:\n",
        "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "                conf = float(box.conf[0].cpu().numpy())\n",
        "\n",
        "                detection = {\n",
        "                    'bbox': [int(x1), int(y1), int(x2), int(y2)],\n",
        "                    'confidence': conf\n",
        "                }\n",
        "\n",
        "                # Add padding\n",
        "                if return_crops:\n",
        "                    bbox_padded = self._add_padding(frame, detection['bbox'], padding)\n",
        "                    detection['face_crop'] = self._crop_face(frame, bbox_padded)\n",
        "                    detection['bbox_padded'] = bbox_padded\n",
        "\n",
        "                detections.append(detection)\n",
        "\n",
        "        return detections\n",
        "\n",
        "    def _add_padding(self, frame, bbox, padding):\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        h, w = frame.shape[:2]\n",
        "\n",
        "        face_w = x2 - x1\n",
        "        face_h = y2 - y1\n",
        "        pad_w = int(face_w * padding)\n",
        "        pad_h = int(face_h * padding)\n",
        "\n",
        "        x1_pad = max(0, x1 - pad_w)\n",
        "        y1_pad = max(0, y1 - pad_h)\n",
        "        x2_pad = min(w, x2 + pad_w)\n",
        "        y2_pad = min(h, y2 + pad_h)\n",
        "\n",
        "        return [x1_pad, y1_pad, x2_pad, y2_pad]\n",
        "\n",
        "    def _crop_face(self, frame, bbox):\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        return frame[y1:y2, x1:x2].copy()\n",
        "\n",
        "    def get_largest_face(self, detections):\n",
        "        \"\"\"Return largest face (primary student)\"\"\"\n",
        "        if not detections:\n",
        "            return None\n",
        "\n",
        "        areas = [(d['bbox'][2] - d['bbox'][0]) * (d['bbox'][3] - d['bbox'][1]) for d in detections]\n",
        "        return detections[np.argmax(areas)]\n",
        "\n",
        "\n",
        "print(\"âœ… YOLOFaceDetector class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section4"
      },
      "source": [
        "## ðŸ§  **4. Vision Transformer Feature Extractor** (Frozen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vit"
      },
      "outputs": [],
      "source": [
        "class ViTFeatureExtractor:\n",
        "    \"\"\"\n",
        "    Vision Transformer for extracting facial features.\n",
        "    FROZEN - no gradients during training.\n",
        "    Used ONLY in Stage A (offline extraction).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name='google/vit-base-patch16-224-in21k', device='cuda'):\n",
        "        self.device = torch.device(device)\n",
        "\n",
        "        print(f\"Loading Vision Transformer: {model_name}\")\n",
        "        self.processor = ViTImageProcessor.from_pretrained(model_name)\n",
        "        self.model = ViTModel.from_pretrained(model_name).to(self.device)\n",
        "        self.model.eval()  # ALWAYS in eval mode\n",
        "\n",
        "        # Freeze all parameters\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.embedding_dim = self.model.config.hidden_size\n",
        "        print(f\"âœ… ViT loaded (FROZEN) | Embedding dim: {self.embedding_dim}\")\n",
        "\n",
        "    def extract_features(self, face_image):\n",
        "        \"\"\"\n",
        "        Extract features from face crop.\n",
        "\n",
        "        Args:\n",
        "            face_image: Face crop (H, W, 3) BGR or PIL Image\n",
        "\n",
        "        Returns:\n",
        "            features: (embedding_dim,) numpy array\n",
        "        \"\"\"\n",
        "        # Convert BGR to RGB\n",
        "        if isinstance(face_image, np.ndarray):\n",
        "            face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\n",
        "            face_image = Image.fromarray(face_image)\n",
        "\n",
        "        # Preprocess\n",
        "        inputs = self.processor(images=face_image, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        # Extract features (no gradients)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            # Use [CLS] token\n",
        "            cls_token = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "\n",
        "        return cls_token.squeeze()\n",
        "\n",
        "\n",
        "print(\"âœ… ViTFeatureExtractor class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section5"
      },
      "source": [
        "## ðŸ”¬ **5. Kolmogorov-Arnold Network (KAN) Layer** (Kept from original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kan"
      },
      "outputs": [],
      "source": [
        "class KANLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Kolmogorov-Arnold Network Layer with learnable B-spline basis functions.\n",
        "\n",
        "    Unlike traditional neural networks with fixed activations,\n",
        "    KAN learns the activation functions as B-splines.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, num_basis=8, spline_order=3, grid_range=(-1, 1)):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.num_basis = num_basis\n",
        "        self.spline_order = spline_order\n",
        "        self.grid_range = grid_range\n",
        "\n",
        "        # Learnable spline coefficients\n",
        "        self.spline_coeffs = nn.Parameter(\n",
        "            torch.randn(in_features, out_features, num_basis) * 0.1\n",
        "        )\n",
        "\n",
        "        # Create B-spline knot vector\n",
        "        num_knots = num_basis + spline_order + 1\n",
        "        internal_knots = num_basis - spline_order + 1\n",
        "\n",
        "        knots = np.concatenate([\n",
        "            np.full(spline_order, grid_range[0]),\n",
        "            np.linspace(grid_range[0], grid_range[1], internal_knots),\n",
        "            np.full(spline_order, grid_range[1])\n",
        "        ])\n",
        "\n",
        "        self.register_buffer('knots', torch.tensor(knots, dtype=torch.float32))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x: (batch_size, in_features)\n",
        "\n",
        "        Returns:\n",
        "            (batch_size, out_features)\n",
        "        \"\"\"\n",
        "        # Normalize input to [-1, 1]\n",
        "        x_normalized = torch.tanh(x)\n",
        "\n",
        "        # Evaluate B-spline basis\n",
        "        basis_values = self._evaluate_bspline_basis(x_normalized)\n",
        "\n",
        "        # Apply coefficients\n",
        "        output = torch.einsum('bik,iok->bo', basis_values, self.spline_coeffs)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _evaluate_bspline_basis(self, x):\n",
        "        \"\"\"\n",
        "        Evaluate B-spline basis functions.\n",
        "\n",
        "        Args:\n",
        "            x: (batch_size, in_features) in [-1, 1]\n",
        "\n",
        "        Returns:\n",
        "            (batch_size, in_features, num_basis)\n",
        "        \"\"\"\n",
        "        batch_size, in_features = x.shape\n",
        "        device = x.device\n",
        "\n",
        "        # Initialize basis matrix\n",
        "        basis = torch.zeros(batch_size, in_features, self.num_basis, device=device)\n",
        "\n",
        "        # Clamp to grid range\n",
        "        x_clamped = torch.clamp(x, self.grid_range[0], self.grid_range[1])\n",
        "\n",
        "        # Simplified polynomial basis\n",
        "        for k in range(self.num_basis):\n",
        "            basis[:, :, k] = x_clamped ** k\n",
        "\n",
        "        # Normalize\n",
        "        basis = F.normalize(basis, p=2, dim=2)\n",
        "\n",
        "        return basis\n",
        "\n",
        "\n",
        "print(\"âœ… KANLayer class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stage_a_header"
      },
      "source": [
        "---\n",
        "# ðŸ”µ **STAGE A: OFFLINE FEATURE EXTRACTION**\n",
        "---\n",
        "\n",
        "**This section runs ONCE to extract embeddings from videos.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section6"
      },
      "source": [
        "## ðŸ“¹ **6. Video Embedding Extraction Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "extract_fn"
      },
      "outputs": [],
      "source": [
        "def extract_video_embeddings(video_path, yolo_detector, vit_extractor,\n",
        "                              max_frames=30, frame_skip=5):\n",
        "    \"\"\"\n",
        "    Extract temporal embeddings from a video.\n",
        "\n",
        "    Args:\n",
        "        video_path: Path to video file\n",
        "        yolo_detector: YOLOFaceDetector instance\n",
        "        vit_extractor: ViTFeatureExtractor instance\n",
        "        max_frames: Maximum frames to extract\n",
        "        frame_skip: Sample every N frames\n",
        "\n",
        "    Returns:\n",
        "        embeddings: numpy array (max_frames, embedding_dim)\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    embeddings = []\n",
        "    frame_idx = 0\n",
        "    frames_extracted = 0\n",
        "\n",
        "    while frames_extracted < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Sample frames\n",
        "        if frame_idx % frame_skip == 0:\n",
        "            # Detect face\n",
        "            detections = yolo_detector.detect_faces(frame, return_crops=True)\n",
        "\n",
        "            if detections:\n",
        "                # Get largest face\n",
        "                face_data = yolo_detector.get_largest_face(detections)\n",
        "                face_crop = face_data['face_crop']\n",
        "\n",
        "                # Extract ViT features\n",
        "                features = vit_extractor.extract_features(face_crop)\n",
        "                embeddings.append(features)\n",
        "                frames_extracted += 1\n",
        "\n",
        "        frame_idx += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Handle empty sequences\n",
        "    if len(embeddings) == 0:\n",
        "        print(f\"WARNING: No faces detected in {video_path}\")\n",
        "        embeddings = [np.zeros(vit_extractor.embedding_dim)]\n",
        "\n",
        "    # Convert to array\n",
        "    sequence = np.array(embeddings)\n",
        "\n",
        "    # Pad to max_frames\n",
        "    if len(sequence) < max_frames:\n",
        "        pad_size = max_frames - len(sequence)\n",
        "        pad = np.zeros((pad_size, vit_extractor.embedding_dim))\n",
        "        sequence = np.vstack([sequence, pad])\n",
        "    else:\n",
        "        sequence = sequence[:max_frames]  # Truncate if too long\n",
        "\n",
        "    return sequence  # Shape: (max_frames, embedding_dim)\n",
        "\n",
        "\n",
        "print(\"âœ… extract_video_embeddings function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section7"
      },
      "source": [
        "## ðŸš€ **7. Batch Extraction Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "batch_extract"
      },
      "outputs": [],
      "source": [
        "def extract_all_embeddings(video_paths, labels, yolo_detector, vit_extractor,\n",
        "                            save_dir='/content/daisee_embeddings',\n",
        "                            max_frames=30, frame_skip=5):\n",
        "    \"\"\"\n",
        "    Extract embeddings for all videos and save to disk.\n",
        "\n",
        "    Args:\n",
        "        video_paths: List of video file paths\n",
        "        labels: List of corresponding labels\n",
        "        yolo_detector: YOLO detector\n",
        "        vit_extractor: ViT extractor\n",
        "        save_dir: Directory to save embeddings\n",
        "        max_frames: Max frames per video\n",
        "        frame_skip: Frame sampling rate\n",
        "\n",
        "    Returns:\n",
        "        metadata: List of dicts with paths and labels\n",
        "    \"\"\"\n",
        "    # Create save directory\n",
        "    save_path = Path(save_dir)\n",
        "    save_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    metadata = []\n",
        "\n",
        "    print(f\"Extracting embeddings for {len(video_paths)} videos...\")\n",
        "\n",
        "    for idx, (video_path, label) in enumerate(tqdm(zip(video_paths, labels),\n",
        "                                                     total=len(video_paths),\n",
        "                                                     desc=\"Extracting\")):\n",
        "        # Extract embeddings\n",
        "        embeddings = extract_video_embeddings(\n",
        "            video_path, yolo_detector, vit_extractor,\n",
        "            max_frames=max_frames, frame_skip=frame_skip\n",
        "        )\n",
        "\n",
        "        # Generate unique ID\n",
        "        video_id = Path(video_path).stem\n",
        "        embedding_file = save_path / f\"{video_id}.npy\"\n",
        "\n",
        "        # Save embeddings\n",
        "        np.save(embedding_file, embeddings)\n",
        "\n",
        "        # Store metadata\n",
        "        metadata.append({\n",
        "            'video_id': video_id,\n",
        "            'embedding_path': str(embedding_file),\n",
        "            'label': label,\n",
        "            'shape': embeddings.shape\n",
        "        })\n",
        "\n",
        "    # Save metadata\n",
        "    metadata_file = save_path / 'metadata.json'\n",
        "    with open(metadata_file, 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    print(f\"\\nâœ… Extraction complete!\")\n",
        "    print(f\"   Saved {len(metadata)} embedding files to: {save_dir}\")\n",
        "    print(f\"   Metadata saved to: {metadata_file}\")\n",
        "\n",
        "    return metadata\n",
        "\n",
        "\n",
        "print(\"âœ… extract_all_embeddings function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section8"
      },
      "source": [
        "## ðŸŽ¬ **8. Run Stage A: Extract Embeddings**\n",
        "\n",
        "**âš ï¸ IMPORTANT: Only run this cell ONCE after collecting your video dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_stage_a"
      },
      "outputs": [],
      "source": [
        "# Initialize YOLO and ViT for extraction\n",
        "print(\"Initializing YOLO and ViT for Stage A...\")\n",
        "yolo_detector = YOLOFaceDetector(device=device)\n",
        "vit_extractor = ViTFeatureExtractor(device=device)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STAGE A: OFFLINE FEATURE EXTRACTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# TODO: Replace with your actual video paths and labels\n",
        "# Example:\n",
        "# video_paths = [\n",
        "#     '/content/videos/student1_confused.mp4',\n",
        "#     '/content/videos/student2_not_confused.mp4',\n",
        "#     # ...\n",
        "# ]\n",
        "# labels = [1.0, 0.0, ...]  # 1.0 = confused, 0.0 = not confused\n",
        "\n",
        "# For demo purposes:\n",
        "print(\"\\nTo run extraction:\")\n",
        "print(\"1. Prepare your video dataset\")\n",
        "print(\"2. Create lists: video_paths and labels\")\n",
        "print(\"3. Run: metadata = extract_all_embeddings(video_paths, labels, yolo_detector, vit_extractor)\")\n",
        "print(\"4. This will save .npy files to /content/daisee_embeddings/\")\n",
        "print(\"\\nExample code:\")\n",
        "print(\"\"\"\n",
        "# video_paths = glob.glob('/content/daisee_videos/*.mp4')\n",
        "# labels = [get_label(v) for v in video_paths]  # Your label extraction logic\n",
        "# metadata = extract_all_embeddings(video_paths, labels, yolo_detector, vit_extractor)\n",
        "\"\"\")\n",
        "\n",
        "# Uncomment when ready:\n",
        "# metadata = extract_all_embeddings(video_paths, labels, yolo_detector, vit_extractor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stage_b_header"
      },
      "source": [
        "---\n",
        "# ðŸŸ¢ **STAGE B: TRAINING & INFERENCE**\n",
        "---\n",
        "\n",
        "**This section trains on pre-extracted embeddings.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section9"
      },
      "source": [
        "## ðŸ“¦ **9. Lightweight Embedding Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset"
      },
      "outputs": [],
      "source": [
        "class EmbeddingDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Lightweight dataset that loads pre-extracted embeddings.\n",
        "\n",
        "    NO video I/O. NO YOLO. NO ViT.\n",
        "    Just loads .npy files.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, metadata_or_paths, embedding_dir=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            metadata_or_paths: Either:\n",
        "                - List of dicts from extract_all_embeddings\n",
        "                - Path to metadata.json file\n",
        "            embedding_dir: Directory containing .npy files (if using json path)\n",
        "        \"\"\"\n",
        "        if isinstance(metadata_or_paths, (str, Path)):\n",
        "            # Load from JSON\n",
        "            with open(metadata_or_paths, 'r') as f:\n",
        "                self.metadata = json.load(f)\n",
        "        else:\n",
        "            self.metadata = metadata_or_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            embeddings: Tensor (max_frames, embedding_dim)\n",
        "            label: Tensor (1,)\n",
        "        \"\"\"\n",
        "        item = self.metadata[idx]\n",
        "\n",
        "        # Load embeddings from disk\n",
        "        embeddings = np.load(item['embedding_path'])\n",
        "\n",
        "        # Convert to tensors\n",
        "        embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32)\n",
        "        label_tensor = torch.tensor([item['label']], dtype=torch.float32)\n",
        "\n",
        "        return embeddings_tensor, label_tensor\n",
        "\n",
        "\n",
        "print(\"âœ… EmbeddingDataset class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section10"
      },
      "source": [
        "## ðŸ§  **10. LSTM + KAN Confusion Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model"
      },
      "outputs": [],
      "source": [
        "class ConfusionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete confusion detection model.\n",
        "\n",
        "    Architecture:\n",
        "        Embeddings (T, 768) â†’ LSTM â†’ KAN â†’ Confusion Score (0-1)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=768, lstm_hidden=256, lstm_layers=2,\n",
        "                 kan_hidden_dims=[128, 64], dropout=0.3,\n",
        "                 num_basis=8, spline_order=3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.lstm_hidden = lstm_hidden\n",
        "\n",
        "        # LSTM for temporal modeling\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if lstm_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        lstm_output_dim = lstm_hidden * 2  # Bidirectional\n",
        "\n",
        "        # KAN layers\n",
        "        self.kan_layers = nn.ModuleList()\n",
        "\n",
        "        prev_dim = lstm_output_dim\n",
        "        for hidden_dim in kan_hidden_dims:\n",
        "            self.kan_layers.append(\n",
        "                KANLayer(prev_dim, hidden_dim, num_basis, spline_order)\n",
        "            )\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        # Final KAN output layer\n",
        "        self.kan_output = KANLayer(prev_dim, 1, num_basis, spline_order)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Output activation\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x: (batch, seq_len, 768) embeddings\n",
        "\n",
        "        Returns:\n",
        "            confusion_score: (batch, 1) in [0, 1]\n",
        "        \"\"\"\n",
        "        # LSTM encoding\n",
        "        lstm_out, _ = self.lstm(x)  # (batch, seq_len, lstm_hidden*2)\n",
        "\n",
        "        # Take last timestep\n",
        "        lstm_last = lstm_out[:, -1, :]  # (batch, lstm_hidden*2)\n",
        "\n",
        "        # KAN layers\n",
        "        x = lstm_last\n",
        "        for kan_layer in self.kan_layers:\n",
        "            x = kan_layer(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        # Output\n",
        "        x = self.kan_output(x)\n",
        "        confusion_score = self.sigmoid(x)\n",
        "\n",
        "        return confusion_score\n",
        "\n",
        "\n",
        "print(\"âœ… ConfusionModel class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section11"
      },
      "source": [
        "## ðŸ‹ï¸ **11. Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training"
      },
      "outputs": [],
      "source": [
        "def train_confusion_model(model, train_loader, val_loader,\n",
        "                           num_epochs=20, learning_rate=0.001,\n",
        "                           loss_type='bce', device='cuda',\n",
        "                           save_path='best_confusion_model.pth'):\n",
        "    \"\"\"\n",
        "    Train confusion detection model.\n",
        "\n",
        "    Args:\n",
        "        model: ConfusionModel instance\n",
        "        train_loader: Training DataLoader\n",
        "        val_loader: Validation DataLoader\n",
        "        num_epochs: Number of epochs\n",
        "        learning_rate: Learning rate\n",
        "        loss_type: 'bce' or 'mse'\n",
        "        device: Device to train on\n",
        "        save_path: Path to save best model\n",
        "\n",
        "    Returns:\n",
        "        model: Trained model\n",
        "        history: Training history dict\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Loss function\n",
        "    if loss_type == 'bce':\n",
        "        criterion = nn.BCELoss()\n",
        "    elif loss_type == 'mse':\n",
        "        criterion = nn.MSELoss()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown loss_type: {loss_type}\")\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
        "    )\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'val_accuracy': []\n",
        "    }\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRAINING START\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
        "        for sequences, labels in train_bar:\n",
        "            sequences = sequences.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward\n",
        "            predictions = model(sequences)\n",
        "            loss = criterion(predictions, labels)\n",
        "\n",
        "            # Backward\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
        "            for sequences, labels in val_bar:\n",
        "                sequences = sequences.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                predictions = model(sequences)\n",
        "                loss = criterion(predictions, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Accuracy\n",
        "                pred_labels = (predictions > 0.5).float()\n",
        "                correct += (pred_labels == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy = correct / total\n",
        "\n",
        "        # Update history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_accuracy'].append(val_accuracy)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Print stats\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "        print(f\"  Val Loss:   {val_loss:.4f}\")\n",
        "        print(f\"  Val Acc:    {val_accuracy:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': val_loss,\n",
        "                'val_accuracy': val_accuracy\n",
        "            }, save_path)\n",
        "            print(f\"  âœ… Best model saved! (val_loss: {val_loss:.4f})\")\n",
        "\n",
        "        print()\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"TRAINING COMPLETE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "print(\"âœ… train_confusion_model function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section12"
      },
      "source": [
        "## ðŸš€ **12. Run Stage B: Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_training"
      },
      "outputs": [],
      "source": [
        "# Create datasets (assumes embeddings already extracted)\n",
        "print(\"Creating datasets from pre-extracted embeddings...\")\n",
        "\n",
        "# Load metadata\n",
        "embedding_dir = Path('/content/daisee_embeddings')\n",
        "metadata_file = embedding_dir / 'metadata.json'\n",
        "\n",
        "if not metadata_file.exists():\n",
        "    print(\"âš ï¸ Metadata file not found! Run Stage A first.\")\n",
        "    print(f\"Expected location: {metadata_file}\")\n",
        "else:\n",
        "    # Load metadata\n",
        "    with open(metadata_file, 'r') as f:\n",
        "        all_metadata = json.load(f)\n",
        "\n",
        "    # Split into train/val (80/20)\n",
        "    split_idx = int(0.8 * len(all_metadata))\n",
        "    train_metadata = all_metadata[:split_idx]\n",
        "    val_metadata = all_metadata[split_idx:]\n",
        "\n",
        "    print(f\"Train samples: {len(train_metadata)}\")\n",
        "    print(f\"Val samples: {len(val_metadata)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = EmbeddingDataset(train_metadata)\n",
        "    val_dataset = EmbeddingDataset(val_metadata)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(\"\\nâœ… Datasets ready!\")\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"\\nInitializing ConfusionModel...\")\n",
        "    confusion_model = ConfusionModel(\n",
        "        input_dim=768,\n",
        "        lstm_hidden=256,\n",
        "        lstm_layers=2,\n",
        "        kan_hidden_dims=[128, 64],\n",
        "        dropout=0.3\n",
        "    ).to(device)\n",
        "\n",
        "    # Model summary\n",
        "    total_params = sum(p.numel() for p in confusion_model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in confusion_model.parameters() if p.requires_grad)\n",
        "    print(f\"\\nModel Parameters:\")\n",
        "    print(f\"  Total: {total_params:,}\")\n",
        "    print(f\"  Trainable: {trainable_params:,}\")\n",
        "\n",
        "    # Train\n",
        "    print(\"\\nStarting training...\")\n",
        "    trained_model, history = train_confusion_model(\n",
        "        model=confusion_model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        num_epochs=20,\n",
        "        learning_rate=0.001,\n",
        "        loss_type='bce',\n",
        "        device=device,\n",
        "        save_path='best_confusion_model.pth'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section13"
      },
      "source": [
        "## ðŸ“Š **13. Plot Training History**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot_history"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training curves.\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Loss curves\n",
        "    ax1.plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
        "    ax1.plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Training & Validation Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Accuracy curve\n",
        "    ax2.plot(history['val_accuracy'], label='Val Accuracy', linewidth=2, color='green')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.set_title('Validation Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"âœ… Training curves saved to training_curves.png\")\n",
        "\n",
        "\n",
        "# Plot if history exists\n",
        "try:\n",
        "    plot_training_history(history)\n",
        "except NameError:\n",
        "    print(\"Run training first to generate history\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section14"
      },
      "source": [
        "## ðŸŽ¯ **14. Inference Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inference"
      },
      "outputs": [],
      "source": [
        "def predict_from_sequence(model, sequence, device='cuda'):\n",
        "    \"\"\"\n",
        "    Run inference on a single embedding sequence.\n",
        "\n",
        "    Args:\n",
        "        model: Trained ConfusionModel\n",
        "        sequence: numpy array (T, 768) or tensor\n",
        "        device: Device\n",
        "\n",
        "    Returns:\n",
        "        confusion_score: float in [0, 1]\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Convert to tensor if needed\n",
        "    if isinstance(sequence, np.ndarray):\n",
        "        sequence = torch.tensor(sequence, dtype=torch.float32)\n",
        "\n",
        "    # Add batch dimension\n",
        "    sequence = sequence.unsqueeze(0).to(device)  # (1, T, 768)\n",
        "\n",
        "    # Inference\n",
        "    with torch.no_grad():\n",
        "        score = model(sequence).item()\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "def predict_from_embedding_file(model, embedding_path, device='cuda'):\n",
        "    \"\"\"\n",
        "    Run inference on a saved embedding file.\n",
        "\n",
        "    Args:\n",
        "        model: Trained ConfusionModel\n",
        "        embedding_path: Path to .npy file\n",
        "        device: Device\n",
        "\n",
        "    Returns:\n",
        "        confusion_score: float in [0, 1]\n",
        "    \"\"\"\n",
        "    embeddings = np.load(embedding_path)\n",
        "    score = predict_from_sequence(model, embeddings, device)\n",
        "    return score\n",
        "\n",
        "\n",
        "print(\"âœ… Inference functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section15"
      },
      "source": [
        "## ðŸ§ª **15. Test Inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_inference"
      },
      "outputs": [],
      "source": [
        "# Test inference on a sample\n",
        "try:\n",
        "    # Load best model\n",
        "    checkpoint = torch.load('best_confusion_model.pth', map_location=device)\n",
        "    confusion_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(\"âœ… Best model loaded\")\n",
        "    print(f\"   Val Loss: {checkpoint['val_loss']:.4f}\")\n",
        "    print(f\"   Val Accuracy: {checkpoint['val_accuracy']:.4f}\")\n",
        "\n",
        "    # Test on a random embedding file\n",
        "    embedding_files = list(embedding_dir.glob('*.npy'))\n",
        "    if embedding_files:\n",
        "        test_file = embedding_files[0]\n",
        "        print(f\"\\nTesting on: {test_file.name}\")\n",
        "\n",
        "        score = predict_from_embedding_file(confusion_model, test_file, device)\n",
        "        print(f\"Predicted confusion score: {score:.4f}\")\n",
        "\n",
        "        if score > 0.7:\n",
        "            print(\"â†’ High confusion detected! âš ï¸\")\n",
        "        elif score > 0.4:\n",
        "            print(\"â†’ Moderate confusion\")\n",
        "        else:\n",
        "            print(\"â†’ Low confusion âœ…\")\n",
        "    else:\n",
        "        print(\"No embedding files found for testing\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Model checkpoint not found. Train the model first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section16"
      },
      "source": [
        "## ðŸ’¾ **16. Save/Load Model Utilities**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_load"
      },
      "outputs": [],
      "source": [
        "def save_complete_model(model, save_path='edusense_complete.pth'):\n",
        "    \"\"\"\n",
        "    Save model with full config for easy loading.\n",
        "    \"\"\"\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'config': {\n",
        "            'input_dim': model.input_dim,\n",
        "            'lstm_hidden': model.lstm_hidden,\n",
        "        }\n",
        "    }, save_path)\n",
        "    print(f\"âœ… Complete model saved to: {save_path}\")\n",
        "\n",
        "\n",
        "def load_complete_model(load_path='edusense_complete.pth', device='cuda'):\n",
        "    \"\"\"\n",
        "    Load model with automatic config restoration.\n",
        "    \"\"\"\n",
        "    checkpoint = torch.load(load_path, map_location=device)\n",
        "\n",
        "    # Recreate model\n",
        "    model = ConfusionModel(\n",
        "        input_dim=checkpoint['config']['input_dim'],\n",
        "        lstm_hidden=checkpoint['config']['lstm_hidden']\n",
        "    ).to(device)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"âœ… Model loaded from: {load_path}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# Save current model\n",
        "try:\n",
        "    save_complete_model(confusion_model, 'edusense_lstm_kan.pth')\n",
        "except NameError:\n",
        "    print(\"Train model first before saving\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "---\n",
        "# ðŸŽ“ **Summary**\n",
        "---\n",
        "\n",
        "## âœ… What We Built\n",
        "\n",
        "### **Stage A: Offline Feature Extraction**\n",
        "- YOLOv8 face detection\n",
        "- ViT feature extraction (frozen)\n",
        "- Saved embeddings to `.npy` files\n",
        "- **Run once per video dataset**\n",
        "\n",
        "### **Stage B: Training & Inference**\n",
        "- Lightweight `EmbeddingDataset` (no video I/O)\n",
        "- LSTM for temporal modeling\n",
        "- KAN for interpretable classification\n",
        "- Fast training on pre-extracted features\n",
        "\n",
        "## ðŸ”‘ Key Advantages\n",
        "\n",
        "1. **Decoupled Architecture**\n",
        "   - Feature extraction separate from training\n",
        "   - Easy to experiment with different models\n",
        "\n",
        "2. **Efficiency**\n",
        "   - No video processing during training\n",
        "   - ViT frozen (no backprop through transformer)\n",
        "   - 10-100x faster training iterations\n",
        "\n",
        "3. **Production-Ready**\n",
        "   - Embeddings can be pre-computed in batch jobs\n",
        "   - Lightweight inference (just LSTM + KAN)\n",
        "   - Easy deployment\n",
        "\n",
        "4. **Modular**\n",
        "   - Can swap LSTM for GRU/Transformer\n",
        "   - Can add more KAN layers\n",
        "   - Can use different ViT models\n",
        "\n",
        "## ðŸ“‚ File Structure\n",
        "\n",
        "```\n",
        "/content/daisee_embeddings/\n",
        "â”œâ”€â”€ video1.npy          # (30, 768) embeddings\n",
        "â”œâ”€â”€ video2.npy\n",
        "â”œâ”€â”€ ...\n",
        "â””â”€â”€ metadata.json       # Labels and paths\n",
        "\n",
        "Models:\n",
        "â”œâ”€â”€ best_confusion_model.pth      # Best checkpoint\n",
        "â”œâ”€â”€ edusense_lstm_kan.pth         # Final model\n",
        "â””â”€â”€ training_curves.png           # Visualizations\n",
        "```\n",
        "\n",
        "## ðŸš€ Next Steps\n",
        "\n",
        "1. âœ… Extract embeddings from your DAiSEE dataset\n",
        "2. âœ… Train LSTM + KAN model\n",
        "3. Fine-tune hyperparameters (LSTM layers, KAN hidden dims)\n",
        "4. Add attention mechanism if needed\n",
        "5. Deploy for real-time confusion detection\n",
        "\n",
        "---\n",
        "\n",
        "**King Khalid University**  \n",
        "**EduSense Graduation Project 2025**  \n",
        "**Production-Ready 2-Stage Architecture âœ…**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
